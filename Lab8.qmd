---
title: "Lab 8: Machine Learning The Whole Game Plan"
subtitle: "Ecosystem Science"
author:
  - name: Madi Schartz
    email: "mmschartz04@gmail.com"
date: "2025-04-04"
format: html
execute: 
  echo: true
---

# Load in necessary packages
```{r}

library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(visdat)
library(skimr)
library (powerjoin)

```

# Read in CAMELS data 

```{r}

# Read in Data from CAMELS data set

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

# Documentation PDF 
download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 'data/camels_attributes_v2.0.pdf')

# Vector storing data types/files
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')

# Where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')

# Download more data

walk2(remote_files, local_files, download.file, quiet = TRUE)

# Read and merge data

camels <- map(local_files, read_delim, show_col_types = FALSE) 

# Full join of data

camels <- power_full_join(camels ,by = 'gauge_id')

```

# Clean Data 

```{r}

# Summary data 
summary(camels)
ls(camels)

# Data cleaning
camels <- na.omit(camels)
camels_clean <- camels %>%
  filter(complete.cases(select(., aridity, p_mean, q_mean))) %>%
  select(aridity, p_mean, q_mean, gauge_lat, gauge_lon)
# Check data for cleanliness
skim(camels_clean)
vis_miss(camels_clean)
```


# Visual Exploratory Data

```{r}

# Map of q mean of the sites

ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  ggthemes::theme_map()



```

# Data Spliting

```{r}

# Start by splitting the data 

set.seed(123)

# Generate the split

camels_split <- initial_split(camels_clean, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

```

# Feature Engineering 

```{r}

# Create a recipe to preprocess the data

library(recipes)
camels_recipe <- recipe(q_mean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_numeric_predictors(), base = 10) %>%
  step_naomit(all_predictors(), all_outcomes())


summary(camels_train)

```

# Resampling and Model Testing

```{r}

# Build re-samples

camels_cv <- vfold_cv(camels_train, v = 10)

# Define Three Regression Models  
linear_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

rf_model <- rand_forest() |> 
  set_engine("ranger") |> 
  set_mode("regression")

xgbm_model <- boost_tree() |> 
  set_engine("xgboost") |> 
  set_mode("regression")


# Workflow Set/Map/Auto-plot
library(workflows)
library(workflowsets)
library(tune)
library(yardstick)

model_workflows <- workflow_set(
  preproc = list(camels_recipe),
  models = list(
    linear = linear_model,
    random_forest = rf_model,
    xgboost = xgbm_model
  )
)

model_results <- model_workflows %>%
  workflow_map("fit_resamples", resamples = camels_cv)

autoplot(model_results)

# Model Selection with Justification 

# Out of the three models, the one that I am going to select that performed the best is the Random Forest model that's set with the ranger engine and regression mode. In our workflow rank it shows that this model had the lowest scores in the mae and rmse but the highest rsq closest to 1 which shows that this model is the best to represent our camels data and predict average streamflow (q_mean) accurately since it's overall a simplistic model with high predictive acccuracy.

```

# Model Tuning 

```{r}

# Define a tunable model 

rf_tune <- rand_forest(
  mode = "regression",
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Check the Tunable Values/Ranges

dials <- extract_parameter_set_dials(rf_tune) 
dials$object

rf_grid <- grid_regular(
  mtry(range = c(1, 5)),
  min_n(range = c(2, 10)),
  levels = 5
)
rf_grid

# Create a workflow

rf_workflow <- workflow() %>%
  add_recipe(camels_recipe) %>%
  add_model(rf_tune)

rf_tune_results <- rf_workflow %>%
  tune_grid(
    resamples = camels_cv,
    grid = rf_grid
  )

autoplot(rf_tune_results)

rf_wf <- workflow() %>%
  add_recipe(camels_recipe) %>%
  add_model(rf_tune)

rf_wf

# Define Search Space (SFD with 25 predefined combos)

dials <- extract_parameter_set_dials(rf_wf)

dials <- update(dials, mtry = mtry(range = c(1, 10)))

dials <- update(dials, min_n = min_n(range = c(2, 15)))

my_grid <- grid_space_filling(dials, size = 25)

dials

dials$object

dials <- extract_parameter_set_dials(rf_workflow)

dials <- finalize(dials, camels_train)

my.grid <- grid_space_filling(dials, size = 25)

my.grid

# Tune the Model 

model_params <-  tune_grid(
    rf_wf,
    resamples = camels_cv,
    grid = my.grid,
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

autoplot(model_params)

# Describe what you see !

# This autoplot shows six different graphs in relation to rmse, rsq, and mae metric values in randomly selected predictors and minimal node size. For the minimal node size,, we can see the metrics of mae and rmse follow a similar pattern with no outliers which shows that this model is consistent and a good choice.

```

# Check the skill of the tuned model 

```{r}

# Check using collect_metric () function

model_params %>% collect_metrics()


# Using this function, I can see from the tibble it produced that all metrics are standard with rsq being the highest at 0.9 and mae being the lowest at 0.32 which is good for our model as there is low error and high correlation between variables.


# Check using the show_best() function

model_params %>%
  show_best(metric = "mae")

# From this function we are able to see that the top five performing models have different amounts of trees and min_n that bring in a mean of 0.32. The best hyper-parameter set is the model 1 with a mtry of 1 and min_n of 24.

# Use the select_best() function to save the best hp

hp_best <- select_best(model_params, metric = "mae")

hp_best
```

# Finalize your model 

```{r}
# Use the Finalize_Workflow () function 

final_wf <- finalize_workflow(rf_wf, hp_best)

print(final_wf)

```

# Final Model Verification

```{r}

# Use last_fit() to finalize workflow

final_fit <- last_fit(final_wf, camels_split)

final_metrics <- collect_metrics(final_fit)
final_metrics

# Interpret the Metrics

# The final model performed very well, similarly to how our random forest model performed with the metrics in standard form of rsq, rmse, and mae, which means it performed well with the test data. The results show that there is a final rmse of 0.4 and a rsq of 0.9 meaning that the predicted values are close to the actual values from the lower rmse metric and 80% of variability in test data due to the high rsq which is a strong model choice. The rsq values being similar on both data sets shows a high correlation for both applications of the model. 


# Plot Predictions

final_predictions <- collect_predictions(final_fit)
final_predictions

ggplot(final_predictions, aes(x = .pred, y = q_mean)) +
  geom_point(aes(color = "green" ), alpha = 0.6) +
  geom_smooth(method = "lm", color = "pink", se = FALSE) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("darkgreen")) +
  labs(
    title = "Predicted vs Actual Values",
    x = "Predicted Streamflow (q_mean)",
    y = "Actual Streamflow (q_mean)",
    caption = "Red line: Linear fit"
  ) +
  theme_minimal()

```

# Building a Map! 

```{r}

# Use the augment() function for predictions

final_model <- fit(final_wf, data = camels_clean)
predictions <- augment(final_model, camels_clean)


# Use the mutate() function to get the residuals 

predictions <- predictions %>%
  mutate(residuals = .pred - q_mean)

head(predictions)

# Map of Predictions 

pred_map <- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) + borders("state", colour = "black", fill = NA) +
  geom_point(alpha = 0.6) +
  scale_color_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Predicted Streamflow (q_mean)", color = "Predicted Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(pred_map)

# Map of Residuals 

resid_map <- ggplot(predictions, aes(x = gauge_lon, y = gauge_lat, color = residuals)) + borders("state", colour = "black", fill = NA) +
  geom_point(alpha = 0.6) +
  scale_color_viridis_c(option = "C") +
  labs(title = "Residuals of Predictions (Predicted - Actual)", color = "Residuals") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(resid_map)

# Use patchwork to combine the tw maps into a figure 

library(patchwork)

combined_map <- pred_map + resid_map +
  plot_layout(ncol = 2, heights = c(6, 6))

print(combined_map)

```

